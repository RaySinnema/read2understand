langchain4j:
  ollama:
    chat-model:
      base-url: http://localhost:11434
      model-name: llama3.1
      timeout: PT30M
      log-requests: false
      log-responses: false

spring:
  server:
    connection-timeout: 180000 # 30 minutes
  servlet:
    multipart:
      max-file-size: 10MB
      max-request-size: 10MB
    session:
      timeout: 30m
  tomcat:
    connection-timeout: ${spring.server.connection-timeout}

logging:
  level:
    dev.langchain4j.model.ollama: DEBUG
